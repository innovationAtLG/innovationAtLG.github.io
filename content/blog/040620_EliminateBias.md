---
title: "Tips & Tricks to Eliminate Bias from User Research Tests"
date: "2020-04-06T12:14:34+06:00"
images: ["images/blog/bPost1_Img-1.png"]
image: "images/blog/bPost1_Img-1.png"
description: "If bias is the number one issue in conducting tests, then can we eliminate bias from user research?  With these tips and tricks, you’ll be able to rid bias from user studies.  These guidelines will help ensure accurate data and credible findings."
author: "Fanya Young"
---

<!-- ![Image-1](/images/blog/bPost1_Img-1.png)  -->
<img class="img-fluid w-100" src="/images/blog/bPost1_Img-1.png" alt="userResearch">

The impact of User
Experience is profound.  In the LG Innovation Lab, the Future Experience
team (or "FX") is tasked with spearheading new and innovative products.
Products designed and developed by the FX team often begin from nothing.
A clean slate. Sounds risky? It can be. However, the FX team decreases
the risk and increases chances of success via User Experience (UX) and
User Experience Research (UXR).

The value of investment in user experience varies from a return of \$2
to \$100 for every \$1 invested in user experience design.  Take
Wal-Mart for example. Wal-Mart experienced a 214% increase of visitors
to their website after optimizing the user experience.  Good design
begins with good research because research provides insights and
actionable recommendations that help product teams make *informed*
design decisions.

User Research is essential to developing products that meet user demands
in today's global market. Understanding the needs of users is so
important in creating a viable product in today's competitive economy.
In fact, global tech companies, like Facebook, employ more than 600 User
Experience Researchers to create, execute, and analyze user tests.  User
tests are no longer optional. User tests are essential to ensure a
product is aligned with user needs.

## Research Done Right

The number one issue in conducting tests is bias.  Bias is a
"prejudice in favor of or against one thing, person, or group
compared with another, usually in a way considered to be unfair."

![Image-2](/images/blog/bPost1_Img-2.png) 

When user tests are written with bias, the
questions are likely to lead participants to one answer over another. 
Data gathered from a test laced with bias compromises the data and the
integrity of the findings. 

                    

>"Is there anything I can do to avoid bias?"  The answer is "Yes!"

Here are three tips that will help non-researchers to avoid bias when
drafting a user test. 

## How to Avoid Bias

Let's examine how one question is biased in three different ways. Let's
assume we work for Company A, a software company. The objective of these
three questions is to get user feedback about Company A's software
compared to their competition.

**Question 1**

> * **1. Which software do you prefer?**
>
> * **a. Company A**
>
> * **b. Other**

Can you identify the problems with the question and answers? Let's see
how well you did.

> **1. Write a specific and clear question that will give you the information you need.**
>
> ![Image-3](/images/blog/bPost1_Img-3.png)
>
> This question is bias two-fold. There's
> bias in the question and bias in the answer. First, the question is
> vague as to the meaning of "preferred."

A better question would be a multiple choice question that clearly
defines both the stated use of the software and the specific companies
in competition with Company A.

> ***"Which software do you prefer to use to \[state the function or use
> \] ?"***
>
> Second, the answer choices of "other" is vague as to the competitors.
> As a result, both the question and answers listed lead users to select
> Company A, the only specific choice that makes sense in such without
> additional context. Thus, bias needs to be removed from the answer
> choices, as well.

> **2. Create Answer Choices That Are Uniform, Standard, and Indistinguishable.**

![Image-4](/images/blog/bPost1_Img-4.png)

There are only two answers: ***Company A and Other***. Naming only one product, the Company A's product, is
extremely biased. No other software is identified as a possible answer. Thus, users are more likely to select Company A software rather than the \"catch all\" term of "other," to represent the competition.

The solution is to give users more than two choices to choose: 

> ***1. "Which software do you prefer to use to \[state the function or
> use \] ?"***
>
> * **c. Company 1**
>
> * **d. Company A**
>
> * **e. Start-up B**
>
> * **f. Corporation C**
>
> * **g. Other: \_\_\_\_\_\_\_\_\_**

There are many reasons to increase the number of options. Doing so will
not lead users to the only brand-name product. Providing a minimum of 4
possible answers will guard against bias. Finally, adding a fill-in the
blank option will solve the problem of users having to choose a stated
choice, rather than giving an honest answer.

The final example of bias is determining whether the question one asks
will elicit the information one seeks. As applied in the current
example, do you want to know the number of users prefer Company A's
software? Or the reasons why the software is preferred?

> **3. Ask Yourself: "Is There a Qualitative Answer for a
> Qualitative Question?" or "A Quantitative Answer for a Quantitative
> Question?"**
>
> ![Image-5](/images/blog/bPost1_Img-5.png)
>
> In the example above, we assumed the goal is to get the names of Company
> A software competitors these users
> currently use.  If one of the products accounts for the majority of
> the votes, then that's a number.  Qualitative data.  If the actual or
> relevant goal is to understand the reasons the users selected that
> software, then we need to ask the users *"Why did they choose their
> answer?  What's so great about the software they currently use?" *in a
> manner that is easily synthesized.

A better solution may be to include a follow-up qualitative question
which asks the users to describe the reason for their selection:

> ***1. "Which software do you prefer to use to \[state the function or
> use \] ?"***
>
> * **c. Company 1**
>
> * **d. Company A**
>
> * **e. Start-up B**
>
> * **f. Corporation C**
>
> * **g. Other: \_\_\_\_\_\_\_\_\_**
>
>
> ***2. Why? Please explain in detail.***

*"Why?"* is a powerful question indeed. This question elicits open-ended
responses, revealing information that is often a treasure trove for
product teams. By allowing users to speak freely, the data collected
will be credible, insightful, and trustworthy. 

It may take time to recognize bias, but you'll be able to catch it with
a little bit of practice. Eliminating bias will give you the data you
want and the answers product teams need!
